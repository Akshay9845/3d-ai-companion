// Definitive Whisper STT - addresses all core issues with bulletproof fallbacks
import { env, pipeline } from '@xenova/transformers';

// Configure transformers.js optimally
env.allowLocalModels = true;
env.allowRemoteModels = true;
env.useBrowserCache = true;
env.backends.onnx.wasm.wasmPaths = 'https://cdn.jsdelivr.net/npm/onnxruntime-web@1.14.0/dist/';

interface WhisperStatus {
  isReady: boolean;
  currentStrategy: string;
  modelName: string;
  error?: string;
  fallbackActive: boolean;
}

class DefinitiveWhisperSTT {
  private pipeline: any = null;
  private isInitialized = false;
  private status: WhisperStatus = {
    isReady: false,
    currentStrategy: 'none',
    modelName: 'none',
    fallbackActive: false
  };
  
  // Web Speech API fallback
  private recognition: any = null;
  private useWebSpeechFallback = false;

  async initialize(): Promise<void> {
    if (this.isInitialized) return;

    console.log('üé§ Initializing Definitive Whisper STT...');
    this.updateStatus('Initializing...', 'initialization');

    try {
      // Always ensure Web Speech API fallback is available
      this.setupWebSpeechFallback();
      
      // Try Whisper with comprehensive strategies
      await this.tryComprehensiveWhisperLoading();
      
      this.isInitialized = true;
      
      if (this.useWebSpeechFallback) {
        this.updateStatus('Ready with Web Speech API', 'web-speech', 'Web Speech API', false, true);
        console.log('‚úÖ Definitive STT ready with Web Speech API fallback');
      } else {
        this.updateStatus('Ready with Whisper', this.status.currentStrategy, this.status.modelName);
        console.log(`‚úÖ Definitive STT ready with Whisper (${this.status.modelName})`);
      }
      
    } catch (error) {
      this.status.error = error.message;
      console.error('‚ùå Definitive STT initialization failed:', error);
      throw error;
    }
  }

  private updateStatus(message: string, strategy: string, modelName: string = 'none', ready: boolean = true, fallback: boolean = false): void {
    this.status = {
      isReady: ready,
      currentStrategy: strategy,
      modelName: modelName,
      error: ready ? undefined : message,
      fallbackActive: fallback
    };
  }

  private setupWebSpeechFallback(): void {
    if (typeof window !== 'undefined' && 'webkitSpeechRecognition' in window) {
      console.log('‚úÖ Web Speech API available as fallback');
      this.recognition = new (window as any).webkitSpeechRecognition();
      this.recognition.continuous = false;
      this.recognition.interimResults = false;
      this.recognition.lang = 'en-US';
    } else if (typeof window !== 'undefined' && 'SpeechRecognition' in window) {
      console.log('‚úÖ Web Speech API available as fallback');
      this.recognition = new (window as any).SpeechRecognition();
      this.recognition.continuous = false;
      this.recognition.interimResults = false;
      this.recognition.lang = 'en-US';
    } else {
      console.warn('‚ö†Ô∏è Web Speech API not available');
    }
  }

  private async tryComprehensiveWhisperLoading(): Promise<void> {
    const strategies = [
      {
        name: 'Verified Remote Model',
        loader: () => this.tryVerifiedRemoteModel()
      },
      {
        name: 'Local Model Check',
        loader: () => this.tryLocalModels()
      },
      {
        name: 'Alternative CDNs',
        loader: () => this.tryAlternativeCDNs()
      },
      {
        name: 'Lightweight Model',
        loader: () => this.tryLightweightModel()
      }
    ];

    for (const strategy of strategies) {
      try {
        console.log(`üîÑ Trying strategy: ${strategy.name}`);
        await strategy.loader();
        
        if (this.pipeline) {
          console.log(`‚úÖ Success with strategy: ${strategy.name}`);
          this.updateStatus('Whisper loaded', strategy.name, this.status.modelName);
          return;
        }
      } catch (error) {
        console.warn(`‚ö†Ô∏è Strategy ${strategy.name} failed:`, error.message);
        continue;
      }
    }

    // If all Whisper strategies fail, use Web Speech API
    console.log('‚ö†Ô∏è All Whisper strategies failed, using Web Speech API');
    if (this.recognition) {
      this.useWebSpeechFallback = true;
      this.updateStatus('Fallback active', 'web-speech-fallback', 'Web Speech API', true, true);
    } else {
      throw new Error('No speech recognition available (Whisper failed, Web Speech API unavailable)');
    }
  }

  private async tryVerifiedRemoteModel(): Promise<void> {
    // Only try the one model we know works
    const modelName = 'Xenova/whisper-tiny.en';
    
    try {
      console.log(`üåê Loading verified model: ${modelName}`);
      
      // First, test if we can reach the model files
      const canReach = await this.testModelReachability(modelName);
      if (!canReach) {
        throw new Error('Model files not reachable');
      }
      
      // Load with optimized settings and timeout
      const loadPromise = pipeline('automatic-speech-recognition', modelName, {
        quantized: true,
        progress_callback: (progress: any) => {
          if (progress.status === 'downloading') {
            const percent = Math.round(progress.progress || 0);
            console.log(`üì• Downloading ${progress.name}: ${percent}%`);
          } else if (progress.status === 'loading') {
            console.log(`üîÑ Loading ${progress.name}...`);
          }
        }
      });
      
      const timeoutPromise = new Promise((_, reject) => 
        setTimeout(() => reject(new Error('Model loading timeout (30s)')), 30000)
      );
      
      this.pipeline = await Promise.race([loadPromise, timeoutPromise]);
      this.status.modelName = modelName;
      this.status.currentStrategy = 'verified-remote';
      
      console.log(`‚úÖ Verified model loaded: ${modelName}`);
      
    } catch (error) {
      console.warn(`Failed to load verified model: ${error.message}`);
      throw error;
    }
  }

  private async testModelReachability(modelName: string): Promise<boolean> {
    try {
      // Test if we can reach the model's config file
      const configUrl = `https://huggingface.co/${modelName}/resolve/main/config.json`;
      
      const controller = new AbortController();
      const timeoutId = setTimeout(() => controller.abort(), 5000);
      
      const response = await fetch(configUrl, { 
        method: 'HEAD',
        signal: controller.signal
      });
      
      clearTimeout(timeoutId);
      
      if (response.ok) {
        console.log(`‚úÖ Model ${modelName} is reachable`);
        return true;
      } else {
        console.warn(`‚ö†Ô∏è Model ${modelName} returned status: ${response.status}`);
        return false;
      }
    } catch (error) {
      console.warn(`‚ö†Ô∏è Model reachability test failed for ${modelName}:`, error.message);
      return false;
    }
  }

  private async tryLocalModels(): Promise<void> {
    // Check for pre-downloaded models
    const localPaths = [
      './models/Xenova/whisper-tiny.en',
      '/models/Xenova/whisper-tiny.en',
      '../models/Xenova/whisper-tiny.en',
      './public/models/Xenova/whisper-tiny.en'
    ];

    for (const path of localPaths) {
      try {
        console.log(`üîç Checking local model at: ${path}`);
        
        this.pipeline = await pipeline('automatic-speech-recognition', path, {
          local_files_only: true
        });
        
        this.status.modelName = `Local: ${path}`;
        this.status.currentStrategy = 'local-model';
        console.log(`‚úÖ Local model loaded: ${path}`);
        return;
        
      } catch (error) {
        console.warn(`Local model not found at ${path}`);
        continue;
      }
    }
    
    throw new Error('No local models found');
  }

  private async tryAlternativeCDNs(): Promise<void> {
    // Try alternative CDN endpoints (though these likely won't work)
    const alternatives = [
      'https://cdn.jsdelivr.net/npm/@xenova/transformers@latest/models/whisper-tiny-en/',
      'https://unpkg.com/@xenova/transformers@latest/models/whisper-tiny-en/'
    ];

    for (const cdnUrl of alternatives) {
      try {
        console.log(`üîÑ Trying alternative CDN: ${cdnUrl}`);
        
        // This is unlikely to work, but worth trying
        const response = await fetch(cdnUrl);
        if (response.ok) {
          console.log(`‚úÖ Alternative CDN accessible: ${cdnUrl}`);
          // Would need to implement custom loading here
        }
        
      } catch (error) {
        console.warn(`Alternative CDN failed: ${cdnUrl}`);
        continue;
      }
    }
    
    throw new Error('All alternative CDNs failed');
  }

  private async tryLightweightModel(): Promise<void> {
    // Try an even smaller model if available
    const lightweightModels = [
      'Xenova/whisper-tiny',
      'microsoft/whisper-tiny'
    ];

    for (const modelName of lightweightModels) {
      try {
        console.log(`üîÑ Trying lightweight model: ${modelName}`);
        
        const canReach = await this.testModelReachability(modelName);
        if (!canReach) continue;
        
        this.pipeline = await pipeline('automatic-speech-recognition', modelName, {
          quantized: true
        });
        
        this.status.modelName = modelName;
        this.status.currentStrategy = 'lightweight';
        console.log(`‚úÖ Lightweight model loaded: ${modelName}`);
        return;
        
      } catch (error) {
        console.warn(`Lightweight model failed: ${modelName}`);
        continue;
      }
    }
    
    throw new Error('All lightweight models failed');
  }

  async transcribe(audioBlob: Blob): Promise<string> {
    if (!this.isInitialized) {
      await this.initialize();
    }

    if (this.useWebSpeechFallback) {
      console.log('üé§ Using Web Speech API for transcription...');
      return await this.transcribeWithWebSpeech(audioBlob);
    }

    if (!this.pipeline) {
      throw new Error('No transcription pipeline available');
    }

    try {
      console.log(`üé§ Transcribing with Whisper (${this.status.modelName})...`);
      
      // Convert blob to audio buffer
      const audioBuffer = await this.blobToAudioBuffer(audioBlob);
      
      // Run Whisper transcription
      const result = await this.pipeline(audioBuffer);
      const transcribedText = result.text || '';
      
      console.log('‚úÖ Whisper transcription completed:', transcribedText);
      return transcribedText;
      
    } catch (error) {
      console.error('‚ùå Whisper transcription failed, trying Web Speech fallback:', error);
      
      if (this.recognition) {
        return await this.transcribeWithWebSpeech(audioBlob);
      } else {
        throw new Error(`Transcription failed: ${error.message}`);
      }
    }
  }

  private async transcribeWithWebSpeech(audioBlob: Blob): Promise<string> {
    if (!this.recognition) {
      throw new Error('Web Speech API not available');
    }

    return new Promise((resolve, reject) => {
      // Create audio URL for playback (Web Speech API needs live audio)
      const audioUrl = URL.createObjectURL(audioBlob);
      const audio = new Audio(audioUrl);
      
      this.recognition.onresult = (event: any) => {
        const transcript = event.results[0][0].transcript;
        console.log('‚úÖ Web Speech API transcription:', transcript);
        URL.revokeObjectURL(audioUrl);
        resolve(transcript);
      };

      this.recognition.onerror = (event: any) => {
        console.error('‚ùå Web Speech API error:', event.error);
        URL.revokeObjectURL(audioUrl);
        reject(new Error(`Web Speech API error: ${event.error}`));
      };

      this.recognition.onend = () => {
        URL.revokeObjectURL(audioUrl);
      };

      // Start recognition
      try {
        this.recognition.start();
        
        // Play audio to trigger recognition (hack for blob-based recognition)
        audio.play().catch(e => console.warn('Audio playback failed:', e));
        
      } catch (error) {
        URL.revokeObjectURL(audioUrl);
        reject(error);
      }
    });
  }

  private async blobToAudioBuffer(blob: Blob): Promise<Float32Array> {
    try {
      const arrayBuffer = await blob.arrayBuffer();
      const audioContext = new AudioContext();
      const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);
      
      // Get mono channel data
      const channelData = audioBuffer.getChannelData(0);
      
      // Resample to 16kHz if needed (Whisper expects 16kHz)
      const targetSampleRate = 16000;
      if (audioBuffer.sampleRate !== targetSampleRate) {
        return this.resampleAudio(channelData, audioBuffer.sampleRate, targetSampleRate);
      }
      
      return new Float32Array(channelData);
    } catch (error) {
      console.error('‚ùå Failed to convert audio blob:', error);
      throw new Error('Failed to process audio data');
    }
  }

  private resampleAudio(audioData: Float32Array, originalSampleRate: number, targetSampleRate: number): Float32Array {
    if (originalSampleRate === targetSampleRate) {
      return audioData;
    }
    
    const ratio = originalSampleRate / targetSampleRate;
    const newLength = Math.round(audioData.length / ratio);
    const result = new Float32Array(newLength);
    
    for (let i = 0; i < newLength; i++) {
      const index = i * ratio;
      const indexFloor = Math.floor(index);
      const indexCeil = Math.min(indexFloor + 1, audioData.length - 1);
      const fraction = index - indexFloor;
      
      result[i] = audioData[indexFloor] * (1 - fraction) + audioData[indexCeil] * fraction;
    }
    
    return result;
  }

  // Public status methods
  isReady(): boolean {
    return this.status.isReady;
  }

  getStatus(): string {
    if (!this.isInitialized) {
      return this.status.error ? `Failed: ${this.status.error}` : 'Not initialized';
    }
    
    if (this.status.fallbackActive) {
      return `Ready (Web Speech API Fallback)`;
    }
    
    return `Ready (Whisper: ${this.status.modelName})`;
  }

  getDetailedStatus(): WhisperStatus {
    return { ...this.status };
  }

  getCurrentStrategy(): string {
    return this.status.currentStrategy;
  }

  isUsingFallback(): boolean {
    return this.status.fallbackActive;
  }
}

// Export singleton instance
export const definitiveWhisperSTT = new DefinitiveWhisperSTT();
export default definitiveWhisperSTT; 